{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "szO16q_1vXOT",
        "pycharm": {}
      },
      "source": [
        "# Attention Based Classification Tutorial\n",
        "\n",
        "**Recommended time: 30 minutes**\n",
        "\n",
        "**Contributors: nthain, martin-gorner**\n",
        "\n",
        "\n",
        "This tutorial provides an introduction to building text classification models in tensorflow that use attention to provide insight into how classification decisions are being made. We will build our tensorflow graph following the Embed - Encode - Attend - Predict paradigm introduced by Matthew Honnibal. For more information about this approach, you can refer to:\n",
        "\n",
        "Slides: https://goo.gl/BYT7au\n",
        "\n",
        "Video: https://youtu.be/pzOzmxCR37I\n",
        "\n",
        "\n",
        "Figure 1 below provides a representation of the full tensorflow graph we will build in this tutorial. The green squares represent RNN cells and the blue trapezoids represent neural networks for computing attention weights which will be discussed in more detail below. We will implement each piece of this model graph in a seperate function. The whole model will then simply be calling all of these functions in turn. \n",
        "\n",
        "\n",
        "![Figure 1](img/entire_model.png \"Figure 1\")\n",
        "\n",
        "This tutorial was created in collaboration with the Tensorflow without a PhD series. To check out more episodes, tutorials, and codelabs from this series, please visit: \n",
        "\n",
        "https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tROhMjW49Dsr",
        "pycharm": {}
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "vSgQlcQqbWyb",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module \u0027tensorflow.python.framework.fast_tensor_util\u0027 does not match runtime version 3.6\n  return f(*args, **kwds)\n",
            "/home/newander/PycharmProjects/conversationai-models/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "%load_ext autoreload\n%autoreload 2\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport os\nfrom sklearn import metrics\nfrom visualize_attention import AttentionDisplay\nfrom process_figshare import download_figshare, process_figshare\n\ntf.set_random_seed(1234)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KKwX66FG9G-L",
        "pycharm": {}
      },
      "source": [
        "## Load \u0026 Explore Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4YFtwZsD4J7r",
        "pycharm": {}
      },
      "source": [
        "Let\u0027s begin by downloading the data from [Figshare](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and cleaning and splitting it for use in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "download_figshare()\n",
        "process_figshare()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We then load these splits as pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "aIy4ggIxbWyg",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "SPLITS \u003d [\u0027train\u0027, \u0027dev\u0027, \u0027test\u0027]\n",
        "\n",
        "wiki \u003d {}\n",
        "for split in SPLITS:\n",
        "    wiki[split] \u003d pd.read_csv(\u0027data/wiki_%s.csv\u0027 % split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_eZEM1wd5FiA",
        "pycharm": {}
      },
      "source": [
        "We display the top few rows of the dataframe to see what we\u0027re dealing with. The key columns are \u0027comment\u0027 which contains the text of a comment from a Wikipedia talk page and \u0027toxicity\u0027 which contains the fraction of annotators who found this comment to be toxic. More information about the other fields and how this data was collected can be found on [this wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) and [research paper](https://arxiv.org/abs/1610.08914).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 195,
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 334,
          "status": "ok",
          "timestamp": 1519755503377,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "6sj_aimNbWyn",
        "outputId": "36fccb7e-60a3-4d1c-bbfa-03483ff49f84",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "wiki[\u0027train\u0027].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p0cz2kA_9JxK",
        "pycharm": {}
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Hyperparameters are used to specify various aspects of our model\u0027s architecture. In practice, these are often critical to model performance and are carefully tuned using some type of [hyperparameter search](https://en.wikipedia.org/wiki/Hyperparameter_optimization). For this tutorial, we will choose a reasonable set of hyperparameters and treat them as fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "JSvJ3wwwbWys",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "hparams \u003d {\u0027max_document_length\u0027: 60,\n",
        "           \u0027embedding_size\u0027: 50,\n",
        "           \u0027rnn_cell_size\u0027: 128,\n",
        "           \u0027batch_size\u0027: 256,\n",
        "           \u0027attention_size\u0027: 32,\n",
        "           \u0027attention_depth\u0027: 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "owTqZg2ebWyv",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "MAX_LABEL \u003d 2\n",
        "WORDS_FEATURE \u003d \u0027words\u0027\n",
        "NUM_STEPS \u003d 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Step 0: Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Before we can build a neural network on comment strings, we first have to complete a number of preprocessing steps. In particular, it is important that we \"tokenize\" the string, splitting it into an array of tokens. In our case, each token will be a word in our sentence and they will be seperated by spaces and punctuation. Many alternative tokenizers exist, some of which use characters as tokens, and others which include punctuation, emojis, or even cleverly handle misspellings. \n",
        "\n",
        "Once we\u0027ve tokenized the sentences, each word will be replaced with an integer representative. This will make the embedding (Step 1) much easier. \n",
        "\n",
        "Happily the tensorflow function [VocabularyProcessor](http://tflearn.org/data_utils/#vocabulary-processor) takes care of both the tokenization and integer mapping. We only have to give it the max_document_length argument which will determine the length of the output arrays. If sentences are shorter than this length, they will be padded and if they are longer, they will be trimmed. The VocabularyProcessor is then trained on the training set to build the initial vocabulary and map the words to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "9kcrgebgbWzB",
        "pycharm": {}
      },
      "outputs": [],
      "source": "# Initialize the vocabulary processor\nvocab_processor \u003d tf.contrib.learn.preprocessing.VocabularyProcessor(hparams[\u0027max_document_length\u0027])\n\ndef process_inputs(vocab_processor, df, train_label \u003d \u0027train\u0027, test_label \u003d \u0027test\u0027):\n    \n    # For simplicity, we call our features x and our outputs y\n    x_train \u003d df[\u0027train\u0027].comment\n    y_train \u003d df[\u0027train\u0027].is_toxic\n    x_test \u003d df[\u0027test\u0027].comment\n    y_test \u003d df[\u0027test\u0027].is_toxic\n\n    # Train the vocab_processor from the training set\n    x_train \u003d vocab_processor.fit_transform(x_train)\n    # Transform our test set with the vocabulary processor\n    x_test \u003d vocab_processor.transform(x_test)\n\n    # We need these to be np.arrays instead of generators\n    x_train \u003d np.array(list(x_train))\n    x_test \u003d np.array(list(x_test))\n    y_train \u003d np.array(y_train).astype(int)\n    y_test \u003d np.array(y_test).astype(int)\n\n    n_words \u003d len(vocab_processor.vocabulary_)\n    print(\u0027Total words: %d\u0027 % n_words)\n\n    # Return the transformed data and the number of words\n    return x_train, y_train, x_test, y_test, n_words\n\nx_train, y_train, x_test, y_test, n_words \u003d process_inputs(vocab_processor, wiki)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1KtFmLmp9M0t",
        "pycharm": {}
      },
      "source": [
        "### Step 1: Embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AjtQe9eT9v4v",
        "pycharm": {}
      },
      "source": [
        "Neural networks at their core are a composition of operators from linear algebra and non-linear activation functions. In order to perform these computations on our input sentences, we must first embed them as a vector of numbers. There are two main approaches to perform this embedding:\n",
        "\n",
        "\n",
        "1.   **Pre-trained:** It is often beneficial to initialize our embedding matrix using pre-trained embeddings like [Word2Vec](??) or [GloVe](??). These embeddings are trained on a huge corpus of text with a general purpose problem so that they incorporate syntactic and semantic properties of the words being embedded and are amenable to transfer learning on new problems. Once initialized, you can optionally train them further for your specific problem by allowing the embedding matrix in the graph to be a trainable variable in our tensorflow graph. \n",
        "2.   **Random:** Alternatively, embeddings can be \"trained from scratch\" by initializing the embedding matrix randomly and then training it like any other parameter in the tensorflow graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rCubiw6eUVQm",
        "pycharm": {}
      },
      "source": [
        "In this notebook, we will be using a random initialization. To perform this embedding we use the embed_sequence function from the layers package. This will take our input features, which are the arrays of integers we produced in Step 0, and will randomly initialize a matrix to embed them into. The parameters of this matrix will then be trained with the rest of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "UG1UXX4L_KQk",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def embed(features):\n",
        "    word_vectors \u003d tf.contrib.layers.embed_sequence(\n",
        "        features[WORDS_FEATURE], \n",
        "        vocab_size\u003dn_words, \n",
        "        embed_dim\u003dhparams[\u0027embedding_size\u0027])\n",
        "    \n",
        "    return word_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBp5uc-tSee2",
        "pycharm": {}
      },
      "source": [
        "### Step 2: Encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9vjxtIroTBUq",
        "pycharm": {}
      },
      "source": [
        "A [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) is a deep learning architecture that is useful for encoding sequential information like sentences. They are built around a single cell which contains one of several standard neural network architectures (e.g. simple [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network), [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit), or [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)). We will not focus on the details of the architectures, but at each point in time the cell takes in two inputs and produces two outputs. The inputs are the input token for that step in the sequence and some state from the previous steps in the sequence. The outputs produced are the encoded vectors for the current sequence step and a state to pass on to the next step of the sequence. \n",
        "\n",
        "Figure 2 shows what this looks like for an unrolled RNN. Each cell (represented by a green square) has two input arrows and two output arrrows. Note that all of the green squares represent the same cell and share parameters. One major advantage of this cell replication is that, at inference time, it allows us to deal with arbitrary length input and not be restricted by the input sizes of our training set.\n",
        "\n",
        "![Figure 2](img/figure_2_v0.png \"Figure 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "For our model, we will use a bi-directional RNN. This is simply the concatentation of two RNNs, one which processes the sequence from left to right (the \"forward\" RNN) and one which process from right to left (the \"backward\" RNN). By using both directions, we get a stronger encoding as each word can be encoded using the context of its neighbors on boths sides rather than just a single side.  For our cells, we use [gated recurrent units (GRUs)](https://en.wikipedia.org/wiki/Gated_recurrent_unit). Figure 3 gives a visual representation of this.\n",
        "\n",
        "![Figure 3](img/figure_3.png \"Figure 3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "DBDS9LjdUZbV",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def encode(word_vectors):\n",
        "    # Create a Gated Recurrent Unit cell with hidden size of RNN_SIZE.\n",
        "    # Since the forward and backward RNNs will have different parameters, we instantiate two seperate GRUS.\n",
        "    rnn_fw_cell \u003d tf.contrib.rnn.GRUCell(hparams[\u0027rnn_cell_size\u0027])\n",
        "    rnn_bw_cell \u003d tf.contrib.rnn.GRUCell(hparams[\u0027rnn_cell_size\u0027])\n",
        "    \n",
        "    # Create an unrolled Bi-Directional Recurrent Neural Networks to length of\n",
        "    # max_document_length and passes word_list as inputs for each unit.\n",
        "    outputs, _ \u003d tf.nn.bidirectional_dynamic_rnn(rnn_fw_cell, \n",
        "                                                 rnn_bw_cell, \n",
        "                                                 word_vectors, \n",
        "                                                 dtype\u003dtf.float32, \n",
        "                                                 time_major\u003dFalse)\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V8hbwTb7dXLV",
        "pycharm": {}
      },
      "source": [
        "### Step 3: Attend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PMKkWgSwdZSq",
        "pycharm": {}
      },
      "source": [
        "There are a number of ways to use the encoded states of a recurrent neural network for prediction. One traditional approach is to simply use the final encoded state of the network, as seen in Figure 2. However, this could lose some useful information encoded in the previous steps of the sequence. In order to keep that information, one could instead use an average of the encoded states outputted by the RNN. There is not reason to believe, though, that all of the encoded states of the RNN are equally valuable. Thus, we arrive at the idea of using a weighted sum of these encoded states to make our prediction.\n",
        "\n",
        "We will call the weights of this weighted sum \"attention weights\" as we will see below that they correspond to how important our model thinks each token of the sequence is in making a prediction decision. We compute these attention weights simply by building a small fully connected neural network on top of each encoded state. This network will have a single unit final layer which will correspond to the attention weight we will assign. As for RNNs, the parameters of this network will be the same for each step of the sequence, allowing us to accomodate variable length inputs. Figure 4 shows us what the graph would look like if we applied attention to a uni-directional RNN.\n",
        "\n",
        "![Figure 4](img/figure_4.png \"Figure 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Again, as our model uses a bi-directional RNN, we first concatenate the hidden states from each RNN before computing the attention weights and applying the weighted sum. Figure 5 below visualizes this step. \n",
        "\n",
        "![Figure 5](img/figure_5.png \"Figure 5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "3a9fkmUOdeHh",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def attend(inputs, attention_size, attention_depth):\n",
        "  \n",
        "  inputs \u003d tf.concat(inputs, axis \u003d 2)\n",
        "  \n",
        "  inputs_shape \u003d inputs.shape\n",
        "  sequence_length \u003d inputs_shape[1].value\n",
        "  final_layer_size \u003d inputs_shape[2].value\n",
        "  \n",
        "  x \u003d tf.reshape(inputs, [-1, final_layer_size])\n",
        "  for _ in range(attention_depth-1):\n",
        "    x \u003d tf.layers.dense(x, attention_size, activation \u003d tf.nn.relu)\n",
        "  x \u003d tf.layers.dense(x, 1, activation \u003d None)\n",
        "  logits \u003d tf.reshape(x, [-1, sequence_length, 1])\n",
        "  alphas \u003d tf.nn.softmax(logits, dim \u003d 1)\n",
        "  \n",
        "  output \u003d tf.reduce_sum(inputs * alphas, 1)\n",
        "\n",
        "  return output, alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bqtYZzWeoz55",
        "pycharm": {}
      },
      "source": [
        "### Step 4: Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To genereate a class prediction about whether a comment is toxic or not, the final part of our tensorflow graph takes the weighted average of hidden states generated in the attention step and uses a fully connected layer with a softmax activation function to generate probability scores for each of our prediction classes. While training, the model will use the cross-entropy loss function to train its parameters. \n",
        "\n",
        "As we will use the [estimator framework](https://www.tensorflow.org/get_started/custom_estimators) to train our model, we write an estimator_spec function to specify how our model is trained and what values to return during the prediction stage. We also specify the evaluation metrics of accuracy and auc, which we will use to evaluate our model in Step 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "L6_Wo4ixbWzI",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def estimator_spec_for_softmax_classification(\n",
        "    logits, labels, mode, alphas):\n",
        "  \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
        "  predicted_classes \u003d tf.argmax(logits, 1)\n",
        "  if mode \u003d\u003d tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode\u003dmode,\n",
        "        predictions\u003d{\n",
        "            \u0027class\u0027: predicted_classes,\n",
        "            \u0027prob\u0027: tf.nn.softmax(logits),\n",
        "            \u0027attention\u0027: alphas\n",
        "        })\n",
        "\n",
        "  onehot_labels \u003d tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
        "  loss \u003d tf.losses.softmax_cross_entropy(\n",
        "      onehot_labels\u003donehot_labels, logits\u003dlogits)\n",
        "  if mode \u003d\u003d tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer \u003d tf.train.AdamOptimizer(learning_rate\u003d0.01)\n",
        "    train_op \u003d optimizer.minimize(loss, \n",
        "                                  global_step\u003dtf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode, \n",
        "                                      loss\u003dloss, \n",
        "                                      train_op\u003dtrain_op)\n",
        "\n",
        "  eval_metric_ops \u003d {\n",
        "      \u0027accuracy\u0027: tf.metrics.accuracy(\n",
        "          labels\u003dlabels, predictions\u003dpredicted_classes),\n",
        "      \u0027auc\u0027: tf.metrics.auc(\n",
        "          labels\u003dlabels, predictions\u003dpredicted_classes),    \n",
        "  }\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode\u003dmode, loss\u003dloss, eval_metric_ops\u003deval_metric_ops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The predict component of our graph then just takes the output of our attention step, i.e. the weighted average of the bi-RNN hidden layers, and adds one more fully connected layer to compute the logits. These logits are fed into a our estimator_spec which uses a softmax to get the final class probabilties and a [softmax_cross_entropy](https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy) to build a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def predict(encoding, labels, mode, alphas):\n",
        "    logits \u003d tf.layers.dense(encoding, MAX_LABEL, activation\u003dNone)\n",
        "    return estimator_spec_for_softmax_classification(\n",
        "          logits\u003dlogits, labels\u003dlabels, mode\u003dmode, alphas\u003dalphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0URRXudn9Qlg",
        "pycharm": {}
      },
      "source": [
        "### Step 5: Complete Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "collapsed": true,
        "id": "cdb9C4jNbCBj",
        "pycharm": {}
      },
      "source": [
        "We are now ready to put it all together. As you can see from the bi_rnn_model function below, once you have the components for embed, encode, attend, and predict, putting the whole graph together is extremely simple!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "FcxSFa5vbWzR",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def bi_rnn_model(features, labels, mode):\n",
        "  \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
        "\n",
        "  word_vectors \u003d embed(features)\n",
        "  outputs \u003d encode(word_vectors)\n",
        "  encoding, alphas \u003d attend(outputs, \n",
        "                            hparams[\u0027attention_size\u0027], \n",
        "                            hparams[\u0027attention_depth\u0027])\n",
        "\n",
        "  return predict(encoding, labels, mode, alphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "![Figure 1](img/entire_model.png \"Figure 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jZqVeWx9TVT",
        "pycharm": {}
      },
      "source": [
        "### Step 6: Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We will use the estimator framework to train our model. To define our classifier, we just provide it with the complete model graph (i.e. the bi_rnn_model function) and a directory where the models will be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "HFDYpImJbWzT",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "current_time \u003d str(int(time.time()))\n",
        "model_dir \u003d os.path.join(\u0027checkpoints\u0027, current_time)\n",
        "classifier \u003d tf.estimator.Estimator(model_fn\u003dbi_rnn_model, \n",
        "                                    model_dir\u003dmodel_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The estimator framework also requires us to define an input function. This will take the input data and provide it during model training in batches. We will use the provided numpy_input_function, which takes numpy arrays as features and labels. We also specify the batch size and whether we want to shuffle the data between epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 34,
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 153379,
          "status": "ok",
          "timestamp": 1519758352944,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "gXJdQHe-bWzX",
        "outputId": "353cbe80-0e36-4832-ed8e-5e6d31087ca1",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Train.\n",
        "train_input_fn \u003d tf.estimator.inputs.numpy_input_fn(\n",
        "  x\u003d{WORDS_FEATURE: x_train},\n",
        "  y\u003dy_train,\n",
        "  batch_size\u003dhparams[\u0027batch_size\u0027],\n",
        "  num_epochs\u003dNone,\n",
        "  shuffle\u003dTrue)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now, it\u0027s finally time to train our model! With estimator, this is as easy as calling the train function and specifying how long we\u0027d like to train for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "classifier.train(input_fn\u003dtrain_input_fn, \n",
        "                 steps\u003dNUM_STEPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wJQI2zW19V8j",
        "pycharm": {}
      },
      "source": [
        "### Step 7: Predict and Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To evaluate the function, we will use it to predict the values of examples from our test set. Again, we define a numpy_input_fn, for the test data in this case, and then have the classifier run predictions on this input function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "4E5poMgPbWza",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Predict.\n",
        "test_input_fn \u003d tf.estimator.inputs.numpy_input_fn(\n",
        "  x\u003d{WORDS_FEATURE: x_test},\n",
        "  y\u003dy_test,\n",
        "  num_epochs\u003d1,\n",
        "  shuffle\u003dFalse)\n",
        "\n",
        "predictions \u003d classifier.predict(input_fn\u003dtest_input_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "These predictions are returned to us as a generator. The code below gives an example of how we can extract the class and attention weights for each prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "oTL7trjX00Zp",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "y_predicted \u003d []\n",
        "alphas_predicted \u003d []\n",
        "for p in predictions:\n",
        "    y_predicted.append(p[\u0027class\u0027])\n",
        "    alphas_predicted.append(p[\u0027attention\u0027])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To evaluate our model, we can use the evaluate function provided by estimator to get the [accuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) and [ROC-AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) scores as we defined them in our estimator_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 34,
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 17936,
          "status": "ok",
          "timestamp": 1519758410784,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "jpgentt6bWzf",
        "outputId": "ae6de3cc-9eb5-469a-e04e-958a784e9dee",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "scores \u003d classifier.evaluate(input_fn\u003dtest_input_fn)\n",
        "print(\u0027Accuracy: {0:f}\u0027.format(scores[\u0027accuracy\u0027]))\n",
        "print(\u0027AUC: {0:f}\u0027.format(scores[\u0027auc\u0027]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOmmwP6UV8h7",
        "pycharm": {}
      },
      "source": [
        "### Step 8: Display Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Now that we have a trained attention based toxicity model, let\u0027s use it to visualize how our model makes its classification decisions. We use the helpful AttentionDisplay class from the visualize_attention package. Given any sentence, this class uses our trained classifier to determine whether the sentence is toxic and also returns a representation of the attention weights. In the arrays below, the more red a word is, the more weight classifier puts on encoded word. Try it out on some sentences of your own and see what patterns you can find!\n\nNote: If you are viewing this on Github, the colors in the cells won\u0027t display properly. We recommend viewing it locally or with [nbviewer](https://nbviewer.jupyter.org/) to see the correct rendering of the attention weights."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "display \u003d AttentionDisplay(vocab_processor, classifier)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1096,
          "status": "ok",
          "timestamp": 1519758417492,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "xSpv2plUV4mN",
        "outputId": "952a6fc6-bac4-46ab-c354-c54e5d288d75",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"Fuck off, you idiot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1024,
          "status": "ok",
          "timestamp": 1519758419192,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "m9bsno-UV4o0",
        "outputId": "beb38261-3e4e-4348-e62f-d23bac629268",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"Thanks for your help editing this.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1223,
          "status": "ok",
          "timestamp": 1519758421016,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "nB4G8rriV4wt",
        "outputId": "2b540ca1-a03d-475a-a54a-6c22558e0be3",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"You\u0027re such an asshole. But thanks anyway.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1067,
          "status": "ok",
          "timestamp": 1519758422814,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "2L3TNl-NV4zV",
        "outputId": "d58ba84a-c30f-4ddb-ecb5-3fc36a850bd5",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"I\u0027m going to shoot you!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1383,
          "status": "ok",
          "timestamp": 1519758424819,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "r5BKahjfV41o",
        "outputId": "05b91277-4d0a-4627-8cb9-c2275a799927",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"Oh shoot. Well alright.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1154,
          "status": "ok",
          "timestamp": 1519758426592,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "8GicGWbCV4uz",
        "outputId": "f02500eb-35a9-466a-a759-8b83fb05feb3",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"First of all who the fuck died and made you the god.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1061,
          "status": "ok",
          "timestamp": 1519758428491,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "kWIR-ivlWi18",
        "outputId": "fb25ede3-e321-4abb-e358-3a0be35266fa",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"Gosh darn it!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1400,
          "status": "ok",
          "timestamp": 1519758433415,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "MJhqEbl8WlJm",
        "outputId": "acf96708-f04a-4493-a650-70ff8f6aa2a7",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"God damn it!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 95,
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1400,
          "status": "ok",
          "timestamp": 1519758437722,
          "user": {
            "displayName": "Nithum Thain",
            "photoUrl": "//lh4.googleusercontent.com/-o8q7BcjxLpg/AAAAAAAAAAI/AAAAAAAAABQ/-zA_Kee6FY0/s50-c-k-no/photo.jpg",
            "userId": "105288052437331023238"
          },
          "user_tz": 210
        },
        "id": "BDWSuL3kZCT1",
        "outputId": "795856d9-ab5d-48aa-ceb2-46a654eec60b",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display.display_prediction_attention(\"You\u0027re not that smart are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "default_view": {},
      "last_runtime": {
        "build_target": "//learning/brain/python/client:colab_notebook",
        "kind": "private"
      },
      "name": "Attention Model Codelab.ipynb",
      "provenance": [
        {
          "file_id": "1TEez0zxlE23RyPtPVEUaL6zhim-r8gMj",
          "timestamp": 1518199421351
        },
        {
          "file_id": "0By5BN4UDRuWSSHJuR2t2YVIzZjQ",
          "timestamp": 1509645017645
        }
      ],
      "version": "0.3.2",
      "views": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}